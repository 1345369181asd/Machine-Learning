## 1 前言 {#1-前言}

在上一篇文章中，我们介绍了基于Bellman方程而得到的Policy Iteration和Value Iteration两种基本的算法，但是这两种算法实际上很难直接应用，原因在于依然是偏于理想化的两个算法，需要知道状态转移概率，也需要遍历所有的状态。对于遍历状态这个事，我们当然可以不用做到完全遍历，而只需要尽可能的通过探索来遍及各种状态即可。而对于状态转移概率，也就是依赖于模型Model，这是比较困难的事情。

什么是状态转移？就比如一颗子弹，如果我知道它的运动速度，运动的当前位置，空气阻力等等，我就可以用牛顿运动定律来描述它的运动，进而知道子弹下一个时刻会大概在哪个位置出现。那么这个基于牛顿运动定律来描述其运动就是一个模型Model，我们也就可以知道其状态（空间位置，速度）的变化概率。

那么基本上所以的增强学习问题都需要有一定的模型的先验知识，至少根据先验知识我们可以来确定需要多少输入可以导致多少输出。比如说玩Atari这个游戏，如果输入只有屏幕的一半，那么我们知道不管算法多么好，也无法训练出来。因为输入被限制了，而且即使是人类也是做不到的。但是以此同时，人类是无需精确的知道具体的模型应该是怎样的，人类可以完全根据观察来推算出相应的结果。

所以，对于增强学习的问题，或者说对于任意的决策与控制问题。输入输出是由基本的模型或者说先验知识决定的，而具体的模型则可以不用考虑。所以，为了更好的求解增强学习问题，我们更关注Model Free的做法。简单的讲就是如果完全不知道状态转移概率（就像人类一样），我们该如何求得最优的策略呢？

本文介绍蒙特卡洛方法。

## 2 蒙特卡洛方法 {#2-蒙特卡洛方法}

蒙特卡洛方法只面向具有阶段episode的问题。比如玩一局游戏，下一盘棋，是有步骤，会结束的。而有些问题则不一定有结束，比如开赛车，可以无限的开下去，或者说需要特别特别久才能结束。能不能结束是一个关键。因为只要能结束，那么每一步的reward都是可以确定的，也就是可以因此来计算value。比如说下棋，最后赢了就是赢了，输了就是输了。而对于结束不了的问题，我们只能对于value进行估计。

那么蒙特卡洛方法只关心这种能够较快结束的问题。蒙特卡洛的思想很简单，就是反复测试求平均。如果大家知道在地上投球计算圆周率的事情就比较好理解了。不清楚的童鞋可以网上找找看。那么如何用在增强学习上呢？

既然每一次的episode都可以到结束，那么意味着根据：

![](/assets/mento-carlo1.png)

