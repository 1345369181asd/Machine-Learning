`A3C`的算法实际上就是将`Actor-Critic`放在了多个线程中进行同步训练. 可以想象成几个人同时在玩一样的游戏, 而他们玩游戏的经验都会同步上传到一个中央大脑. 然后他们又从中央大脑中获取最新的玩游戏方法.

**这样, 对于这几个人, 他们的好处是:**中央大脑汇集了所有人的经验, 是最会玩游戏的一个, 他们能时不时获取到中央大脑的必杀招, 用在自己的场景中.

**对于中央大脑的好处是:**中央大脑最怕一个人的连续性更新, 不只基于一个人推送更新这种方式能打消这种连续性. 使中央大脑不必有用像`DQN`,`DDPG`那样的记忆库也能很好的更新.

![](/assets/reinforcement-a3c.png)为了达到这个目的, 我们要有两套体系, 可以看作中央大脑拥有`global net`和他的参数, 每位玩家有一个`global net`

的副本`local net`, 可以定时向`global net`推送更新, 然后定时从`global net`那获取综合版的更新.

