## 1.多臂赌博机 {#1多臂赌博机}

多臂赌博机是指一类问题，这类问题重复的从k个行为\(action\)中选择一个，并获得一个奖励\(reward\)，一次选择的时间周期称为一个时间步\(time-step\)。当选择并执行完一个行为后，得到一个奖励，我们称奖励的期望为这次行为的**真实值\(value\)**。在t时刻选择的行为用At表示，对应的奖励用Rt表示，对于行为a，其真实值为q∗\(a\),表示行为a的期望奖励，即：

$$q_*(a)=E[R_t|A_t=a]$$

如果我们知道每个行为的真实值，那么多臂赌博机的问题很容易就可以解决，但在大多数情况下，我们是不知道行为的具体值的，因此只能做近似。在t时刻用$$Q_t(a)$$作为$$q_*(a)$$的**估计值**，即$$Q_t(a)$$约等于$$q_*(a)$$

在时刻t，我们可以利用已有的知识即行为的估计值进行行为的最优选择，这种操作称为_exploit_，如果不选择当前的最优行为，我们称这种操作为_explore_，explore操作能够提高对行为值估计的准确度。exploit操作能够最大化当前步的奖励，但explore操作可能会使长期的奖励更大。如何平衡exploit操作和explore操作是强化学习中的一个重要问题。

## 2.估计行为值的方法 {#2估计行为值的方法}

 对行为值的估计是为了更好的选择行为。行为的值为每次执行该行为所得奖励的期望。因此可以用t时刻前行为已得到的奖励作为行为值的估计，即：

![](/assets/multibandit-svgreward.png)

这只是估计行为值的一种方法，但并不是唯一同时也并不是最好的方法。这种方法称为

**样本平均\(sample-average\)法**，在t时刻选择行为时，使用贪心策略选择行为值最大的行为，即

$$A_t=arg_amaxQ_t(a)$$

这种选择行为的方法有一个缺陷，因为每次只做exploit操作，而不做explore操作，所以在选择行为时可能会漏掉那些真实值更大的行为。对此方法的一个改进是在大多数时间进行贪心策略也即exploit操作，但是会以ϵ的概率进行explore操作，也就是以ϵ的概率从其余行为\(不具有最高估计值的行为\)中随机选择一个执行，这种方法称为**ϵ−greedy**方法，这种方法的一个好处是在有限的时间步内能更新所有行为的估计值，即Qt\(a\)能够收敛到qt\(a\)





