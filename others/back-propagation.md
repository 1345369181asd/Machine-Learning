卷积神经网络其实是神经网络特征学习的一个典型例子。传统的机器学习算法其实需要人工的提取特征，比如很厉害的SVM。而卷积神经网络利用模板算子的参数也用以学习这个特点，把特征也学习出来了。其实不同的模板算子本质上就是抽象了图像的不同方面的特征。比如提取边缘，提取梯度的算子。用很多卷积核去提取，那就是 提取了很多的特征。一旦把参数w，b训练出来，意味着特征和目标之间的函数就被确定。今天分享下CNN的关键部分，训练方法，即随机梯度下降和误差反向传播。

先说下推导的思路：

（1）说明CNN是一种局部连接和权值共享的网络，这种网络可以看做是特殊的全连接网络，即不连接部分的权值为0。

（2）解释网上常见的pooling层向卷积层，以及卷积层向pooling层的残差反向传播的原理。

在推导CNN反向传播之前，MLP的反向传播是推导基础。

首先定义$$w_{ij}^{(l)}$$为第l层第j个神经元与第l+1层第i个神经元的链接权重，$$z_i^{(l)}$$为第l层第i个神经元的输入，$$a_i^{(l)}$$为第l层第i个神经元的输出，$$b_i^{(l)}$$为第l+1层第i个神经元的偏置，定义$$h_{wb}(x)$$为神经网络在输入层为x时输出层的输出值，定义函数f为神经元的激活函数。神经元网络的前馈传播可表示成
$$
z_i^{(l+1)}=w^{(l)}a^{(l)}+b^{(l)}
$$

$$
a^{(l+1)}=f(z^{(l+1)}
$$
在网络初始化的时候，可以采用高斯分布的随机函数去初始化权重w，然后定义损失函数（loss function）如下：
$$
J(x^{(i)},y^{(i)};w,b)=\frac {1}{2}||h_{wb}(x^{(i)}-y^{(i)}||^2
$$

$$
J(w,b)=\frac {1}{m}\sum_{i=1}^{m}J(x^{(i)},y^{(i)};w,b)+\frac{\lambda }{2}\sum_{l=1}^{n_l-1}\sum_{i=1}^{s_l+1}\sum_{j}^{s_l}(w_{ij}^l)^2
$$
如果J\(w,b\)加权重的平方，是为了减小权重的大小，防止过拟合。神经网络的训练目标就是寻找合适的w,b使得损失函数J\(w,b\)最小。因而可以使用梯度下降法确定w,b.

$$w_{ij}^{(l)}=w{ij}^{(l)}-\frac{\partial J(w,b)}{\partial w_{ij}^{(l)}}$$

$$w_{ij}^{(l)}=w{ij}^{(l)}-\frac {1}{m}\sum_{i=1}^{m}\frac{\partial J(x^{(i)},y^{(i)};w,b)}{\partial w_{ij}^{(l)}}-\lambda w_{ji}^{(l)}$$

$$w_{ij}^{(l)}=w{ij}^{(l)}-\frac {1}{m}\sum_{i=1}^{m}\frac{\partial J(x^{(i)},y^{(i)};w,b)}{\partial z_{ij}^{(l+1)}}\frac{\partial z_{ij}^{(l+1)}}{\partial w_{ij}^{(l)}}-\lambda w_{ji}^{(l)}$$

$$Qz_{ij}^{(l+1)}=\sum_{k=1}^{sl}w_{ik}^{(l)}a_k^{(l)}+b_i^{(l)}$$

$$\therefore w_{ij}^{(l)}=w{ij}^{(l)}-\frac {1}{m}\sum_{i=1}^{m}\frac{\partial J(x^{(i)},y^{(i)};w,b)}{\partial z_{ij}^{(l+1)}}a_{j}^{(l)}-\lambda w_{ji}^{(l)}$$

$$\therefore b_i^{(l)}=b_i{(l)}-\frac {1}{m}\sum_{i=1}^{m}\frac{\partial J(x^{(i)},y^{(i)};w,b)}{\partial z_{ij}^{(l+1)}}$$





