## Relu 激励函数

![](/assets/commonf-rleu.png)

tanh 激励函数的图

![](/assets/commonf-tanh.png)

## argmax

argmax是一种函数，函数y=f\(x\)，x0= argmax\(f\(x\)\) 的意思就是参数x0满足f\(x0\)为f\(x\)的最大值；换句话说就是 argmax\(f\(x\)\)是使得 f\(x\)取得最大值所对应的变量x。arg即argument，此处意为“自变量”。

## **交叉熵（Cross Entropy）**

是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度（perplexity）来衡量。交叉熵的意义是用该模型对文本识别的难度，或者从压缩的角度来看，每个词平均要用几个位来编码。复杂度的意义是用该模型表示这一文本平均的分支数，其倒数可视为每个词的平均概率。平滑是指对没观察到的N元组合赋予一个概率值，以保证词序列总能通过语言模型得到一个概率值。通常使用的平滑技术有 [图灵估计](https://baike.baidu.com/item/图灵估计/8011740)、删除插值平滑、Katz平滑和Kneser-Ney平滑。

将交叉熵引入计算语言学消岐领域，采用语句的真实语义作为交叉熵的训练集的[先验信息](https://baike.baidu.com/item/先验信息)，将机器翻译的语义作为测试集后验信息。计算两者的交叉熵，并以交叉熵指导对歧义的辨识和消除。实例表明，该方法简洁有效．易于计算机自适应实现。交叉熵不失为计算语言学消岐的一种较为有效的工具。

在信息论中，交叉熵是表示两个概率分布p,q，其中p表示真实分布，q表示非真实分布，在相同的一组事件中，其中，用非真实分布q来表示某个事件发生所需要的平均比特数。从这个定义中，我们很难理解交叉熵的定义。下面举个例子来描述一下：

假设现在有一个样本集中两个概率分布p,q，其中p为真实分布，q为非真实分布。假如，按照真实分布p来衡量识别一个样本所需要的编码长度的期望为：

$$\mathrm{H}(\mathrm{p})=\sum_{i} p(i) \cdot \log \left(\frac{1}{p(i)}\right) |$$

但是，如果采用错误的分布q来表示来自真实分布p的平均编码长度，则应该是：

$$\mathrm{H}(\mathrm{p}, \mathrm{q})=\sum_{i} p(i) \cdot \log \left(\frac{1}{q(i)}\right)$$

此时就将H\(p,q\)称之为交叉熵。交叉熵的计算方式如下：

对于离散变量采用以下的方式计算：$$\mathrm{H}(\mathrm{p}, \mathrm{q})=\sum_{x} p(x) \cdot \log \left(\frac{1}{q(x)}\right)$$

对于连续变量采用以下的方式计算：

$$-\int_{X} P(x) \log Q(x) d r(x)=E_{p}[-\log Q]$$

### 应用

交叉熵可在神经网络\(机器学习\)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

在特征工程中，可以用来衡量两个随机变量之间的相似度。

在语言模型中（NLP）中，由于真实的分布p是未知的，在语言模型中，模型是通过训练集得到的，交叉熵就是衡量这个模型在测试集上的正确率。

## **softmax 函数**

softmax函数可以对上溢和下溢进行数值稳定。softmax 函数经常用于预测与multinoulli分布相关联的概率，定义为：

$$P\left(y_{i}=k | \boldsymbol{x}_{i}\right)=\frac{e^{\boldsymbol{w}_{k} x_{i}+b}}{1+\sum_{k=1}^{k=K-1} e^{w_{k} x_{i}+b}}, k=1,2, \ldots, K-1$$

softmax 函数在多分类问题中非常常见。这个函数的作用就是使得在负无穷到0的区间趋向于0，在0到正无穷的区间趋向于1。上面表达式其实是多分类问题中计算某个样本$$x_i$$的类别标签$$y_i$$属于K个类别的概率，最后判别$$y_i$$所属类别时就是将其归为对应概率最大的那一个。

