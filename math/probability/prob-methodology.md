## 1.统计学习


统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，也称统计机器学习。统计学习是数据驱动的学科。统计学习是一门概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科。


统计学习的对象是数据，它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提。

统计学习的目的就是考虑学习什么样的模型和如何学习模型。

统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法。实现统计学习的步骤如下：

（1）      得到一个有限的训练数据集合；

（2）      确定包含所有可能的模型的假设空间，即学习模型的集合；

（3）      确定模型选择的准则，即学习的策略；

（4）      实现求解最优模型的算法，即学习的算法；

（5）      通过学习方法选择最优模型；

（6）      利用学习的最优模型对新数据进行预测或分析。


## 2.监督学习


监督学习从训练数据中学习模型，对测试数据进行预测，训练集通常表示为


![](http://img.my.csdn.net/uploads/201212/20/1356004383_5770.jpg)

![](http://img.my.csdn.net/uploads/201212/20/1356004415_4974.jpg)


人们根据输入、输出变量的不同类型，对预测任务给予不同的名称：输入变量和输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题。

   监督学习假设输入与输出的随机变量X和Y遵循联合概率分布P\(X,Y\)，P\(X,Y\)表示分布函数，或分布密度函数。统计学习假设数据存在一定的统计规律，X和Y具有联合概率分布的假设就是监督学习关于数据的基本假设。

   监督学习的模型可以是概率模型或非概率模型，由条件概率分布P\(Y\|X\)或决策函数Y=f\(X\)表示，随具体学习方法而定。

  监督学习分为学习和预测两个过程，由学习系统与预测系统组成，如下图：


![](http://img.my.csdn.net/uploads/201212/20/1356004513_5839.jpg)


学习过程中，学习系统利用给定的训练数据集，通过学习得到一个模型，表示为条件概率分布P\(Y\|X\)或决策函数Y=f\(X\)。预测过程中，预测系统对于给定的测试样本集中的输入![](http://img.my.csdn.net/uploads/201212/20/1356004624_2522.jpg)


## 3.统计学习三要素


统计学习=模型+策略+算法


### 3.1 模型


   统计学习中，首先要考虑学习什么样的模型，在监督学习中，模型就是所要学习的条件概率分布或决策函数，由决策函数表示的模型为非概率模型，由条件概率分布表示的模型为概率模型。


![](http://img.my.csdn.net/uploads/201212/20/1356004758_6572.jpg)

### 3.2 策略


有了模型的假设空间，统计学习接着需要考虑的是按照什么样的准则学习或选择最优的模型。监督学习实际上就是一个经验风险或者结构风险函数的最优化问题。风险函数度量平均意义下模型预测的好坏，模型每一次预测的好坏用损失函数来度量。

   监督学习问题就是从假设空间F中选择模型f作为决策函数，对于给定的输入X，由f\(X\)给出相应的输出Y，这个输出的预测值f\(X\)与真实值Y可能一致也可能不一致，用一个损失函数来度量预测错误的程度。损失函数记为L\(Y, f\(X\)\)。常用的损失函数有以下几种：


![](http://img.my.csdn.net/uploads/201212/20/1356004878_7579.jpg)

![](http://img.my.csdn.net/uploads/201212/20/1356004934_9694.jpg)

### 3.3 算法


   统计学习问题归结为以上的最优化问题，这样，统计学习的算法就是求解最优化问题的算法。如果最优化问题有显示的解析解，这个最优化问题就比较简单，但通常这个解析解不存在，所以就需要利用数值计算的方法来求解。统计学习可以利用已有的最优化算法，也可以开发独自的最优化算法。


## 4. 模型评估与模型选择


   当损失函数给定时，基于损失函数的模型的训练误差和模型的测试误差就自然成为学习方法评估的标准。

   训练误差是模型Y=f\(x\)关于训练数据集的平均损失：


![](http://img.my.csdn.net/uploads/201212/20/1356005074_4861.jpg)

![](http://img.my.csdn.net/uploads/201212/20/1356005128_2359.jpg)


下图给出了M=0，M=1，M=3，M=9时的多项式函数拟合的情况，其中绿色曲线为真模型，红色为预测模型。
```

![](http://img.my.csdn.net/uploads/201212/20/1356005203_7128.jpg)

其中，M=0和M=1模型简单，拟合不足，训练误差较大；M=9模型复杂，过拟合，训练误差为0，但基本不具备推广性；M=3模型复杂度适中，泛化能力强，效果最好。


下图描述了训练误差和测试误差与模型的复杂度之间的关系：


![](http://img.my.csdn.net/uploads/201212/20/1356005252_2139.jpg)


当模型的复杂度增大时，训练误差会逐渐减小并趋向于0，而测试误差会先减少，达到最小值后又增大。模型选择的典型方法是正则化与交叉验证。


## 5.正则化与交叉验证


模型选择的典型方法是正则化，正则化的一般形式如下：


![](http://img.my.csdn.net/uploads/201212/20/1356005383_2973.jpg)


其中，第一项是经验风险，第二项是正则化项，正则化项可以取不同的形式，例如，正则化项可以是模型参数向量的范数。回归问题中，损失函数是平方损失，正则化项可以是参数向量的L2范数：


![](http://img.my.csdn.net/uploads/201212/20/1356005410_1407.jpg)

正则化项也可以是参数向量的L1范数：

![](http://img.my.csdn.net/uploads/201212/20/1356005440_7330.jpg)

经验风险较小的模型可能较复杂，这时正则化项的值会较大，正则化的作用是选择经验风险与模型复杂度同时较小的模型。


   正则化符合奥卡姆剃刀原理，在所有可能的模型中，能够很好的解释已知数据并且十分简单的模型才是最好的模型。从贝叶斯估计的角度来看，正则化项对应于模型的先验概率，可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。

模型选择的另一种方法是交叉验证，使用交叉验证的前提是数据不充足，常见的有简单交叉验证、S折交叉验证和留一交叉验证。如果数据充足，选择模型的一种简单方法是随机的将数据集分成三部分，分别为训练集、验证集和测试集，训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。如果数据不充足，可以采用交叉验证的方法来选择模型。


## 6. 泛化能力

![](http://img.my.csdn.net/uploads/201212/20/1356005533_4611.jpg)

## 7.生成模型与判别模型

判别模型

   该模型主要对p\(y\|x\)建模，通过x来预测y。在建模的过程中不需要关注联合概率分布。只关心如何优化p\(y\|x\)使得数据可分。通常，判别式模型在分类任务中的表现要好于生成式模型。但判别模型建模过程中通常为有监督的，而且难以被扩展成无监督的。

   常见的判别式模型有：

Logisticregression

Lineardiscriminant analysis

Supportvector machines

Boosting

Conditionalrandom fields

Linearregression

Neuralnetworks


生成模型


该模型对观察序列的联合概率分布p\(x,y\)建模，在获取联合概率分布之后，可以通过贝叶斯公式得到条件概率分布。生成式模型所带的信息要比判别式模型更丰富。除此之外，生成式模型较为容易的实现增量学习。

   常见的生成式模型有:

Gaussian mixture model and othertypes of mixture model

HiddenMarkov model

NaiveBayes

AODE

LatentDirichlet allocation

RestrictedBoltzmann Machine

   由上可知，判别模型与生成模型的最重要的不同是，训练时的目标不同，判别模型主要优化条件概率分布，使得x,y更加对应，在分类中就是更可分。而生成模型主要是优化训练数据的联合分布概率。而同时，生成模型可以通过贝叶斯得到判别模型，但判别模型无法得到生成模型。


## 8.分类问题、标注问题和回归问题


前面提到过，输入变量和输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题。

   对于二分类问题，常用的评价指标是精确率和召回率。通常以关注的类为正类，其他类为负类，分类器在测试数据集上的预测或正确或不正确，4中情况出现的总数分别记为：

TP——将正类预测为正类数；

FN——将正类预测为负类数；

FP——将负类预测为正类数；

TN——将负类预测为负类数。


则，精确率定义为：

![](http://img.my.csdn.net/uploads/201212/20/1356005736_1884.jpg)


  许多统计方法可以用于分类，包括k近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯谛回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。

   标注问题的输入是一个观测序列，输出是一个标记序列。标注问题在信息抽取、自然语言处理等领域被广泛采用。例如，自然语言处理中的词性标注就是一个典型的标注问题：给定一个由单词组成的句子，对这个句子中的每一个单词进行词性标注，即对一个单词序列预测其对应的词性标记序列。标注常用的统计学习方法有：隐马尔科夫模型、条件随机场。

   回归问题的学习等价于函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好地预测未知数据。回归问题按照输入变量的个数分为一元回归和多元回归，按照输入变量和输出变量之间的关系的类型即模型的类型，分为线性回归和非线性回归。回归学习最常用的损失函数时平方损失函数，在此情况下，回归问题可以用著名的最小二乘法求解。




