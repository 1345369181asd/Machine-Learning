# 第二章 概率论和信息论 {#第三章-概率论和信息论}

这一章主要是讨论概率论和信息论相关的内容。 概率论是一种用来表示不确定状态的数学方法。在人工智能当中，概率论的应用主要在两个方面：1. 概率论的laws告诉我们AI系统如何去完成推论，也就是用来设计AI的推论结构；2. 可以采用概率论和统计理论来理论的分析所提出的人工智能系统的性能。 信息理论的作用就是让我们来量化一个概率分布当中的不确定性。

## 为什么采用概率论 {#为什么采用概率论}

```
 在机器学习当中，算法必须总是处理不确定量以及随机量，同时硬件中的错误也会经常发生。不确定性的来源有三个：
 1. 系统的固有不确定性被建模包含。例如，大多数的量子力学将亚原子的运动描述为不确定性的。
 2. 不可完全观测。即使是确定性系统也会表现出随机性，因为我们肯能不能观察到所有的驱动这个系统的变量。在Monty Hall问题中，输出结果已经给定了，但是对于contestant来讲，输出却是不确定的，有点类似于薛定谔的猫。
 3. 不完整的建模。例如在机器人中，为了估计机器人的位置从而离散空间，这样就没有办法精确的知道objects的位置。
 在很多情况下，一个简单的不确定的系统比复杂的确定的系统更好用。需要指出的一点是，虽然我们想找到一种描述不确定的方法，但是概率论并没所有我们需要的所有工具。概率论最先是用来研究事物发生的频率的，通常是对于可重复事件，重复多次，就会发生那么多次，例如概率为p，可能会有p次发生。但是对于举出的医生的例子，认为flu的概率是40%，这里就是指的置信度的概念（degree of belief）。前一种被称作为频率论的概率论Frequentist probability，后一种用来衡量不确定性水平的被称作为贝叶斯概率论（Bayesian probability）。

```

## 随机变量 {#随机变量}

```
  A random variable is a variable that can take on diﬀerent values randomly.

```

## 概率分布 {#概率分布}

* 离散随机变量和概率质量分布函数
* 连续变量和概率密度函数
* 边际概率分布
* 条件概率分布
* 条件概率的链式法则
* 独立事件和条件独立事件
* 期望、方差、协方差
* ## 常见的概率分布 {#常见的概率分布}
* Bernoulli Distribution 伯努利分布
* Multinoulli Distribution 绝对分布
* Gaussian Distribution 正态分布
* Exponential and Laplace Distributions 指数分布和拉普拉斯分布
* The Dirac Distribution and Empirical Distribution 狄拉克分布和经验分布
* Mixtures of Distributions 混合分布
* ## 一些有用的函数 {#一些有用的函数}
* logistics sigmoid
* softplus function
* ## 贝叶斯规则 {#贝叶斯规则}

主要是和条件概率相关的一些事情

## 连续变量的技术细节 {#连续变量的技术细节}

并没有仔细看这一章，应该是连续变量和离散变量之间的区别（以后修正）

## Information Theory {#information-theory}

```
信息理论是应用数学的一个分支，主要是来围绕着定量的分析a signal中包含了多少信息。最开始是用来研究通过噪声通道发送离散字母的信息发送问题，例如通过无线电的通信。**在这个context中，信息论能告诉我们怎么就优化编码，计算预期的消息长度并从特定采样的概率分布使用不同的编码方案，在机器学习当中，可以将信息论应用于连续变量，消息的长度解释（message length interpretations）并未应用于这些变量的情况下**
采用这种正式的直觉（intuition）
* 相似的事件拥有较少的信息量，在极端情况下，事件确定发生的情况，应该不包含
* 相似度小的事情应该拥有更高的信息量
* 独立事件应该有附加信息

```

为了满足这三个性质，所以呢定义了self-information I\(X\)=-LogP\(x\)**（自信息（英语：self-information），又译为信息本体，由克劳德·香农提出，用来衡量单一事件发生时所包含的信息量多寡）**。用nat作为单位，1nat就是观测可能性为1/e的事件所包含的信息量。可以用Shannon entropy（香农熵）来衡量概率分布当中的不确定性大小。当变量是连续变量时，香龙熵就变成了（differencial entropy）微分熵。如果说遇到了两个分布，那么用Kullback-Leibler \(KL\) divergence来衡量两个分布之间的不同。+

# Structured Probabilistic Models-结构化的概率模型 {#structured-probabilistic-models-结构化的概率模型}

机器学习通常涉及非常大量的随机变量的概率分布。通常，这些概率分布会涉及相对较少变量的直接相互作用。采用一个单一的函数来描述整个联合概率分布式非常有效的（不论是计算效率还是统计效率）。当我们用一个graph来表征概率分布的factorization，就可以称之为structured probabilistic model 结构化概率模型 or graphical model 图论模型。结构概率模型分为两种：直接和间接。这两种方法都是用一个图，每个图的节点表示一个变量，连接点之间的边表示这两个随机变量之间的直接相关的概率分布。

