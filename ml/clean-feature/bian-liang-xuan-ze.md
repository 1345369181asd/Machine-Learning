变量选择是特征工程中非常重要的一部分。特征工程是一个先升维后降维的过程。升维的过程是结合业务理解尽可能多加工特征，是一个非常耗时且需要发散思维的过程。而变量选择就是降维的过程，因为传统评分卡模型为了保证模型的稳定性与Logistic回归模型的准确性，往往对入模变量有非常严格的考量，并希望入模变量最好不要超过20个，且要与业务强相关。这时变量选择显得尤为重要，特征加工阶段往往可以得到几百维或更高的特征，而从中保留最有意义的特征也是重要且耗时的过程。

变量选择的方法常用的有过滤法（Filter）、包装法（Wrapper）、嵌入法（Embedding），并且在上述方法中又有单变量选择、多变量选择、有监督选择、无监督选择。注意在实际应用中，单纯从数据挖掘的角度进行变量选择是不够的，还要结合业务理解对选择后的变量进行回测，以符合业务解释，即变量选择的最终结果还要具有业务解释性，脱离了具体业务而得到的变量选择结果，即使可以达到较好的预测效果，也要进一步推敲。





在变量选择前，特征工程的变量衍生过程中很容易通过各种特征加工方法得到成百上千的特征，如果是一个标准评分卡，其建模变量不会超过20个，通常会将多种变量选的方法进行组合以达到最终的目的。一般的过程如下：

1）基于IV值进行初步筛选，给定一个阈值，一般0.02，进行第一步筛选。

2）聚类分析，将不同特征进行聚类得到不同的簇。目的是在接下来的变量中，在选择剔除变量时要有簇的概念，优先从不同的簇中删除变量。聚类的损失函数是组内间距越小越好、组间间距越大越好，聚类结果会得到不同的簇，但并没有区分哪些簇是正向特征或是负向特征，并且尽量保证每个簇内经过变量选择后都有变量存在，这样可以保证入模变量是从不同维度综合考虑的结果。

3）相关性分析，计算变量编码后变量之间的相关性，给定阈值进行变量剔除，剔除时要注意在不同的簇中进行选择。相关系数的阈值可以选择0.6左右。另外也可以做方差膨胀因子分析，剔除变量。

4）逐步回归stepwise regression变量选择。逐步回归是较好的一种变量选择方法，可以通过前向、后向或双向的方法进行变量选择。

5）随机森林或XGBOOST模型变量重要性排序，得到最终个变量筛选结果。注意，随机森林或Xgboost模型的变量选择并没有考虑变量间的相关性问题，所以需要先过相关性或多重共线性筛选后再进行变量重要性排序。通过累计贡献率的方式，或只选择TOP20的变量即可。当然，也不要忽略不同簇的问题，尽量从多维度保留入模变量的多样性。

